{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN+m3jZHDvOe8Kvx4jyXhJS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sharuhi/haruhi/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HPMWDT0F3bu1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "positional_emb = True\n",
        "conv_layers = 2\n",
        "projection_dim = 128\n",
        "\n",
        "num_heads = 2\n",
        "transformer_units = [\n",
        "    projection_dim,\n",
        "    projection_dim,\n",
        "]\n",
        "transformer_layers = 2\n",
        "stochastic_depth_rate = 0.1\n",
        "\n",
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 128\n",
        "num_epochs = 30\n",
        "image_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CCTTokenizer(layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        padding=1,\n",
        "        pooling_kernel_size=3,\n",
        "        pooling_stride=2,\n",
        "        num_conv_layers=conv_layers,\n",
        "        num_output_channels=[64, 128],\n",
        "        positional_emb=positional_emb,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(CCTTokenizer, self).__init__(**kwargs)\n",
        "\n",
        "        # This is our tokenizer.\n",
        "        self.conv_model = keras.Sequential()\n",
        "        for i in range(num_conv_layers):\n",
        "            self.conv_model.add(\n",
        "                layers.Conv2D(\n",
        "                    num_output_channels[i],\n",
        "                    kernel_size,\n",
        "                    stride,\n",
        "                    padding=\"valid\",\n",
        "                    use_bias=False,\n",
        "                    activation=\"relu\",\n",
        "                    kernel_initializer=\"he_normal\",\n",
        "                )\n",
        "            )\n",
        "            self.conv_model.add(layers.ZeroPadding2D(padding))\n",
        "            self.conv_model.add(\n",
        "                layers.MaxPool2D(pooling_kernel_size, pooling_stride, \"same\")\n",
        "            )\n",
        "\n",
        "        self.positional_emb = positional_emb\n",
        "\n",
        "    def call(self, images):\n",
        "        outputs = self.conv_model(images)\n",
        "        # After passing the images through our mini-network the spatial dimensions\n",
        "        # are flattened to form sequences.\n",
        "        reshaped = tf.reshape(\n",
        "            outputs,\n",
        "            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n",
        "        )\n",
        "        return reshaped\n",
        "\n",
        "    def positional_embedding(self, image_size):\n",
        "        # Positional embeddings are optional in CCT. Here, we calculate\n",
        "        # the number of sequences and initialize an `Embedding` layer to\n",
        "        # compute the positional embeddings later.\n",
        "        if self.positional_emb:\n",
        "            dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
        "            dummy_outputs = self.call(dummy_inputs)\n",
        "            sequence_length = tf.shape(dummy_outputs)[1]\n",
        "            projection_dim = tf.shape(dummy_outputs)[-1]\n",
        "\n",
        "            embed_layer = layers.Embedding(\n",
        "                input_dim=sequence_length, output_dim=projection_dim\n",
        "            )\n",
        "            return embed_layer, sequence_length\n",
        "        else:\n",
        "            return None"
      ],
      "metadata": {
        "id": "I6v7pjff4JE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " dummy_inputs = tf.ones((1, image_size, image_size, 3))\n",
        " dummy_inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "712E6yB289cL",
        "outputId": "0b54b065-d2e9-4bd0-bca1-afcdc9e7c73e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 32, 32, 3), dtype=float32, numpy=\n",
              "array([[[[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        ...,\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         ...,\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (1, 96, 96, 3)\n",
        "x = tf.random.normal(input_shape)\n",
        "y = tf.keras.layers.Conv2D( 64, 3, activation='relu', padding=\"same\",input_shape=input_shape[1:])(x)\n",
        "outputs = tf.keras.layers.Conv2D( 128, 3, activation='relu', padding=\"same\",input_shape=input_shape[1:])(y)\n",
        "print(outputs.shape)\n",
        "\n",
        "reshaped = tf.reshape(\n",
        "            outputs,\n",
        "            (-1, tf.shape(outputs)[1] * tf.shape(outputs)[2], tf.shape(outputs)[-1]),\n",
        "        )\n",
        "print(reshaped)\n",
        "print(tf.shape(outputs)[1])\n",
        "sequence_length = tf.shape(reshaped)[1]\n",
        "print(sequence_length)\n",
        "projection_dim = tf.shape(reshaped)[-1]\n",
        "print(projection_dim)\n",
        "embed_layer = layers.Embedding(\n",
        "                input_dim=sequence_length, output_dim=projection_dim\n",
        "            )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml4mL94K95TM",
        "outputId": "fe9c553c-e481-422d-eaed-1da4d9189009"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 96, 96, 128)\n",
            "tf.Tensor(\n",
            "[[[8.17381591e-03 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "   0.00000000e+00 1.26460984e-01]\n",
            "  [8.62396881e-02 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "   1.12511218e-04 1.93105936e-01]\n",
            "  [1.86059579e-01 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "   0.00000000e+00 1.20073855e-01]\n",
            "  ...\n",
            "  [6.71509281e-02 0.00000000e+00 7.97495022e-02 ... 9.73359793e-02\n",
            "   0.00000000e+00 6.25941157e-02]\n",
            "  [0.00000000e+00 0.00000000e+00 5.49881272e-02 ... 0.00000000e+00\n",
            "   0.00000000e+00 0.00000000e+00]\n",
            "  [1.06742613e-01 7.16863200e-03 1.40815318e-01 ... 0.00000000e+00\n",
            "   5.05825393e-02 2.97512859e-02]]], shape=(1, 9216, 128), dtype=float32)\n",
            "tf.Tensor(96, shape=(), dtype=int32)\n",
            "tf.Tensor(9216, shape=(), dtype=int32)\n",
            "tf.Tensor(128, shape=(), dtype=int32)\n"
          ]
        }
      ]
    }
  ]
}